---
title: "RSM"
output: html_document
---
```{r}
#Phase I
data=read.csv("data/2k.csv",header=T)
y=data$Browse.Time
model=lm(y~Prev.Length*Prev.Size*Tile.Size,data=data)
summary(model)

#ME plots
options(warn=-1)
library(gplots)
plotmeans(formula = y~Prev.Length, ylab = "average  browsing time", xlab = "Preview Length", ylim=c(17.5,20.5),data = data, xaxt = "n", pch = 16)
axis(side = 1, at = c(1,2), labels = c(30, 90))

plotmeans(formula = y~Tile.Size, ylab = "average  browsing time", xlab = "Tile Size", ylim=c(17.5,20.5),data = data, xaxt = "n", pch = 16)
axis(side = 1, at = c(1,2), labels = c(0.1, 0.3))

plotmeans(formula = y~Prev.Size, ylab = "average  browsing time", xlab = "Preview Size", ylim=c(17.5,20.5),data = data, xaxt = "n", pch = 16)
axis(side = 1, at = c(1,2), labels = c(0.3, 0.5))

```


```{r}
#Phase II
## Load helpful packages and functions
library(plot3D) # needed for the mesh() function

# Function to create blues
blue_palette <- colorRampPalette(c(rgb(247,251,255,maxColorValue = 255), rgb(8,48,107,maxColorValue = 255)))

# Function for converting from natural units to coded units
convert.N.to.C <- function(U,UH,UL){
  x <- (U - (UH+UL)/2) / ((UH-UL)/2)
  return(x)
}

# Function for converting from coded units to natural units
convert.C.to.N <- function(x,UH,UL){
  U <- x*((UH-UL)/2) + (UH+UL)/2
  return(U)
}

data2=read.csv("data/p2r.csv",header=T)
table(data2$Prev.Length,data2$Prev.Size)

## Determine whether we're close to the optimum to begin with
## (i.e, check whether the pure quadratic effect is significant)
ph1 <- data.frame(y = data2$Browse.Time,
                  x1 = convert.N.to.C(U = data2$Prev.Length, UH = 90, UL = 30),
                  x2 = convert.N.to.C(U = data2$Prev.Size, UH = 0.5, UL = 0.3))
ph1$xPQ <- (ph1$x1^2 + ph1$x2^2)/2

## Check the average browsing time in each condition:
aggregate(ph1$y, by = list(x1 = ph1$x1, x2 = ph1$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph1$y[ph1$xPQ != 0]) - mean(ph1$y[ph1$xPQ == 0])


## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph1)
summary(m)
############################################################
## steepest descent
m.fo <- lm(y~x1+x2, data = ph1)
beta0 <- coef(m.fo)[1]
beta1 <- coef(m.fo)[2]
beta2 <- coef(m.fo)[3]
grd <- mesh(x = seq(convert.N.to.C(U = 30, UH = 90, UL = 30), 
                    convert.N.to.C(U = 120, UH = 90, UL = 30), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0.1, UH = 0.5, UL = 0.3), 
                    convert.N.to.C(U = 0.8, UH = 0.5, UL = 0.3), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.fo <- beta0 + beta1*x1 + beta2*x2
# 2D contour plot
contour(x = seq(convert.N.to.C(U = 30, UH = 90, UL = 30), 
                convert.N.to.C(U = 120, UH = 90, UL = 30), 
                length.out = 100),
        y = seq(convert.N.to.C(U = 0.1, UH = 0.5, UL = 0.3), 
                convert.N.to.C(U = 0.8, UH = 0.5, UL = 0.3), 
                length.out = 100), 
        z = eta.fo, xlab = "x1 (Preview Length)", ylab = "x2 (Preview Size)",
        nlevels = 15, col = blue_palette(15), labcex = 0.9, asp=0.25)
abline(a = 0, b = beta2/beta1, lty = 2)
points(x = 0, y = 0, col = "red", pch = 16)


# The gradient vector
g <- matrix(c(beta1, beta2), nrow = 1)

# We will take steps of size 5 seconds in preview length. In coded units this is
PL.step <- convert.N.to.C(U = 60 + 5, UH = 90, UL = 30)
lamda <- PL.step/abs(beta1)

## Step 0: The center point we've already observed
x.old <- matrix(0, nrow=1, ncol=2)
text(x = 0, y = 0+0.25, labels = "0")
step0 <- data.frame(Prev.Length = convert.C.to.N(x = 0, UH = 90, UL = 30), 
                 Prev.Size = convert.C.to.N(x = 0, UH = 0.5, UL = 0.3))

## Step 1: 
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "1")
step1 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 2: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "2")
step2 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 3: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "3")
step3 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 4: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "4")
step4 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 5: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "5")
step5 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 6: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "6")
step6 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 7: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "7")
step7 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 8: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "8")
step8 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))
## Step 9: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "9")
step9 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))
## Step 10: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "10")
step10 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 11: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "11")
step11 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## Step 12: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "12")
step12 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, UL = 0.3))

## The following is a list of the conditions along the path of steepest descent
pstd.cond <- data.frame(Step = 0:12, rbind(step0, step1, step2, step3, step4, step5, step6,step7,step8,step9,step10,step11,step12))
pstd.cond


##########Find MOI for each
netflix.ph2 <- read.csv("data/pstd.csv", header = TRUE)

## Calculate the average browsing time in each of these conditions and find the 
## condition that minimizes it
pstd.means <- aggregate(netflix.ph2$Browse.Time, 
                        by = list(Prev.Length = netflix.ph2$Prev.Length, 
                                  Prev.Size = netflix.ph2$Prev.Size), 
                        FUN = mean)

pstd.cond2 <- data.frame(Step = 0:8, rbind(step0, step1, step2, step3, step4, step5, step6,step7,step8),ABT=pstd.means$x)
pstd.cond2

plot(x = 0:8, y = pstd.means$x,
     type = "l", xlab = "Step Number", ylab = "Average Browsing Time")
points(x = 0:8, y = pstd.means$x,
       col = "red", pch = 16)




pstd.cond[pstd.cond$Step == 6,]
##############################################
## Load this data and check whether the pure quadratic effect is significant
netflix.ph2.5 <- read.csv("data/2^2+cp_second_time.csv", header = TRUE)
ph2.5 <- data.frame(y = netflix.ph2.5$Browse.Time,
                  x1 = convert.N.to.C(U = netflix.ph2.5$Prev.Length, UH = 90, UL = 60),
                  x2 = convert.N.to.C(U = netflix.ph2.5$Prev.Size, UH = 0.8, UL = 0.6))
ph2.5$xPQ <- (ph2.5$x1^2 + ph2.5$x2^2)/2

## Check the average browsing time in each condition:
aggregate(ph2.5$y, by = list(x1 = ph1$x1, x2 = ph1$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph2.5$y[ph2.5$xPQ != 0]) - mean(ph2.5$y[ph2.5$xPQ == 0])

## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph2.5)
summary(m)

```


```{r}
#####
#Phase III
## Central Composite Design Example

## Load helpful packages and functions
library(plot3D) # needed for the mesh() function

convert.C.to.N(1.41,75,105)
convert.C.to.N(-1.41,75,105)
convert.C.to.N(1.41,0.4,0.6)
convert.C.to.N(-1.41,0.4,0.6)


lyft <- read.csv("data/lyft.csv", header = TRUE)
table(lyft$Prev.Length,lyft$Prev.Size)


lyft <- data.frame(y = lyft$Browse.Time,
                  Prev.Length = convert.N.to.C(U = lyft$Prev.Length, UH = 90, UL = 60),
                  Prev.Size = convert.N.to.C(U = lyft$Prev.Size, UH = 0.8, UL = 0.6))


## We then fit the full 2nd-order response surface
model <- lm(y ~ Prev.Length + Prev.Size + Prev.Length*Prev.Size + I(Prev.Length^2) + I(Prev.Size^2), data = lyft)
summary(model)

## Let's visualize this surface:
beta0 <- coef(model)[1]
beta1 <- coef(model)[2]
beta2 <- coef(model)[3]
beta12 <- coef(model)[6]
beta11 <- coef(model)[4]
beta22 <- coef(model)[5]
grd <- mesh(x = seq(convert.N.to.C(U = 75, UH = 105, UL = 75), 
                    convert.N.to.C(U = 105, UH = 105, UL = 75), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0.4, UH = 0.6, UL = 0.4), 
                    convert.N.to.C(U = 0.6, UH = 0.6, UL = 0.4), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.so <- beta0 + beta1*x1 + beta2*x2 + beta12*x1*x2 + beta11*x1^2 + beta22*x2^2

# 2D contour plot (coded units)
contour(x = seq(convert.N.to.C(U = 75, UH = 105, UL = 75), 
                    convert.N.to.C(U = 105, UH = 105, UL = 75), 
                length.out = 100), 
        y = seq(convert.N.to.C(U = 0.4, UH = 0.6, UL = 0.4), 
                    convert.N.to.C(U = 0.6, UH = 0.6, UL = 0.4), 
                length.out = 100), 
        z = eta.so, xlab = "x1", ylab = "x2",
        nlevels = 20, col = blue_palette(20), labcex = 0.9)

## Let's find the maximum of this surface and the corresponding factor levels 
## at which this is achieved
b <- matrix(c(beta1,beta2), ncol = 1)
B <- matrix(c(beta11, 0.5*beta12, 0.5*beta12, beta22), nrow = 2, ncol = 2)
x.s <- -0.5*solve(B) %*% b 
points(x = x.s[1], y = x.s[2], col = "red", pch = 16)


# The predicted book rate at this configuration is:
eta.so.opt=beta0+beta1*x.s[1]+beta2*x.s[2]+beta12*x.s[1]*x.s[2]+beta11*x.s[1]^2+beta22*x.s[2]^2
eta.so.opt

# In natural units this optimum is located at
convert.C.to.N(x = x.s[1,1], UH = 105, UL = 75)
convert.C.to.N(x = x.s[2,1], UH = 0.6, UL = 0.4)


# Remake the contour plot but in natural units
contour(x = seq(75, 105, length.out = 100), 
        y = seq(0.4, 0.6, length.out = 100), 
        z = eta.so, xlab = "Discount Amount (%)", ylab = "Discount Duration (Days)",
        nlevels = 20, col = blue_palette(20), labcex = 0.9)

points(x = convert.C.to.N(x = x.s[1,1], UH = 105, UL = 75),
       y = convert.C.to.N(x = x.s[2,1], UH = 0.6, UL = 0.4), 
       col = "red", pch = 16)


```



